# -*- coding: utf-8 -*-
"""Crypto_Momentum_Prediction.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gy2mZNzraLMlMbFf4zPErfUsYdxDCNGc
"""

#Importing necessary Libraries
import pandas as pd
import numpy as np
import datetime as dt
import yfinance as yf
import matplotlib.pyplot as plt
from pandas_datareader import data as pdr
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

!pip install ccxt

pip install python-binance

pip install ta

import ccxt
import pandas as pd
print(ccxt.exchanges)

kucoin=ccxt.kucoin() #Extracting data from kucoin
training_data=pd.DataFrame(kucoin.fetchOHLCV('BTC/USDT',timeframe='1h'))
training_data.columns=['Time','Open','High','Low','Close','Volume']
training_data['Time'] = pd.to_datetime(training_data['Time'], unit='ms')

training_data['returns'] = ((training_data['Close'] / training_data['Open']) - 1) * 100 #calculating returns for each hour
#training_data.head()

def compute_stats(training_data, n):
    rolling_returns = training_data['returns'].rolling(window=n).mean() #calculating mean
    std_dev = training_data['returns'].rolling(window=n).std() #calculating std deviation
    return rolling_returns, std_dev

def label_overreaction(training_data, n, k):
    rolling_returns, std_dev = compute_stats(training_data, n)
    training_data['overreaction'] = np.where(training_data['returns'] > rolling_returns + k * std_dev, 1,
                                    np.where(training_data['returns'] < rolling_returns - k * std_dev, -1, 0)) #calculating overreaction based on formula
    return training_data
a=label_overreaction(training_data,30,1)
a

import pandas as pd
import matplotlib.pyplot as plt

a['Time'] = pd.to_datetime(a['Time'])

plt.figure(figsize=(10, 6))
plt.plot(a['Time'], a['Close'], color='blue', linewidth=1) # Plot the closing prices
plt.title('Hourly Closing Prices Over Time')
plt.xlabel('Time')
plt.ylabel('Closing Price')
plt.xticks(rotation=45)  # Rotating x-axis labels
plt.show()

import pandas as pd
from ta import add_all_ta_features
# calculating MovingAvg,RSI,Macd etc.
training_data = add_all_ta_features(training_data, open='Open', high='High', low='Low', close='Close', volume='Volume', fillna=True)
training_data = training_data.dropna()
training_data.head()

kucoin=ccxt.kucoin()
training_data=pd.DataFrame(kucoin.fetchOHLCV('BTC/USDT',timeframe='1h'))
training_data.columns=['Time','Open','High','Low','Close','Volume']
training_data['Time'] = pd.to_datetime(training_data['Time'], unit='ms')
training_data['returns'] = ((training_data['Close'] / training_data['Open']) - 1) * 100


def compute_stats(training_data, n):
    rolling_returns = training_data['returns'].rolling(window=n).mean()
    std_dev = training_data['returns'].rolling(window=n).std()
    return rolling_returns, std_dev

def label_overreaction(training_data, n, k):
    rolling_returns, std_dev = compute_stats(training_data, n)
    training_data['overreaction'] = np.where(training_data['returns'] > rolling_returns + k * std_dev, 1,
                                            np.where(training_data['returns'] < rolling_returns - k * std_dev, -1, 0))
    return training_data

a = label_overreaction(training_data,30, 1)
a

print(a.count())
columns_to_handle_outliers=['Open','High',	'Low',	'Close',	'Volume',	'returns',	'overreaction']
for i in columns_to_handle_outliers:
    Q1 = a[i].quantile(0.25)
    Q3 = a[i].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = (a[i] < lower_bound) | (a[i] > upper_bound)
    a.loc[outliers, i] = None

processed_data= a.dropna()
processed_data.count()

"""Random Forest Classifier"""

from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Handle missing values
imputer = SimpleImputer(strategy='mean')
x_imputed = imputer.fit_transform(x)

# Standardize the features
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x_imputed)

# Split the data into training and test sets
x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.3, random_state=50)

# Initialize and fit RandomForestClassifier
r_f_classifier = RandomForestClassifier(n_estimators=40, random_state=50)
r_f_classifier.fit(x_train, y_train)

# Make predictions
y_pred = r_f_classifier.predict(x_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", round(accuracy * 100, 2), "%")

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

x = training_data.iloc[:, 1:-1].values
y = training_data.iloc[:,-1].values

imputer = SimpleImputer(strategy='mean')
x_imputed = imputer.fit_transform(x)

pca=PCA(0.95)
x_pca=pca.fit_transform(x_imputed)
pca.explained_variance_ratio_

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.3, random_state=50)

r_f_classifier = RandomForestClassifier(n_estimators=40, random_state=50)
r_f_classifier.fit(x_train, y_train)

y_pred = r_f_classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: ",round(accuracy*100,2),"%")

from sklearn.impute import SimpleImputer

# Handle missing values
imputer = SimpleImputer(strategy='mean')
x_imputed = imputer.fit_transform(x)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x_imputed, y, test_size=0.3, random_state=50)

# Fit the RandomForestClassifier
r_f_classifier = RandomForestClassifier(n_estimators=40, random_state=50)
r_f_classifier.fit(x_train, y_train)

# Predict target labels for the test data
y_pred = r_f_classifier.predict(x_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", round(accuracy * 100, 2), "%")

"""KNN algorithm"""

from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

x = processed_data.iloc[:, 1:-1].values
y = processed_data.iloc[:,-1].values

pca = PCA(0.9)
x_pca = pca.fit_transform(x)
pca.explained_variance_ratio_

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x_pca)
x_train_scaled, x_test_scaled, y_train, y_test = train_test_split(x_scaled, y, test_size=0.3, random_state=50)

x_train_pca, x_test_pca, y_train, y_test = train_test_split(x_pca, y, test_size=0.3, random_state=50)
knn_classifier = KNeighborsClassifier()
knn_classifier.fit(x_train_pca, y_train)

y_pred = knn_classifier.predict(x_test_pca)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of KNN classifier:", round(accuracy*100, 2), "%")

"""Support Vector Machines"""

from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score

x = processed_data.iloc[:, 1:-1].values
y = processed_data.iloc[:,-1].values

classifier=svm.SVC(kernel='rbf',gamma='scale',C=2,random_state=50)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=50)
classifier.fit(x_train,y_train)
y_predict=classifier.predict(x_test)
accuracy=accuracy_score(y_test,y_predict)
print('Accuracy of SVM:',round(accuracy*100,2),'%')

